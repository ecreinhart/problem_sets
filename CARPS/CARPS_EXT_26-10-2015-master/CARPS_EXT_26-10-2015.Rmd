---
title: "CARPS Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

[PILOT/COPILOT - TEXT IN SQUARE BRACKETS IS HERE FOR GUIDANCE. COPILOT PLEASE DELETE BEFORE KNITTING THE FINAL REPORT]

# Report Details

[PILOT/COPILOT ENTER RELEVANT REPORT DETAILS HERE]

```{r}
articleID <- "26-10-2015" # insert the article ID code here e.g., "10-3-2015_PS"
reportType <- "pilot" # specify whether this is the 'pilot' report or 'final' report
pilotNames <- "Ellen Reinhart" # insert the pilot's name here e.g., "Tom Hardwicke".  If there are multiple cpilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
copilotNames <- "Griffin Dietz" # # insert the co-pilot's name here e.g., "Michael Frank". If there are multiple co-pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
pilotTTC <- 270 # insert the pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
copilotTTC <- 60 # insert the co-pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
pilotStartDate <- 10/30/2018 # insert the pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
copilotStartDate <- 10/30/2018 # insert the co-pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
completionDate <- NA # copilot insert the date of final report completion (after any necessary rounds of author assistance) in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
```

------

#### Methods summary: 

[PILOT/COPILOT write a brief summary of the methods underlying the target outcomes written in your own words]

------

#### Target outcomes: 

> Inasmuch as our hypothesis was focused on concurrent
retrieval processes that primarily affect SSRIF, we
conducted separate repeated measures analyses of variance
to test for rehearsal effects and for SSRIF effects. In
the analysis testing for a rehearsal effect, risk condition
was a between-subjects variable, retrieval type (Rp+ vs.
Nrp) was a within-subjects variable, and proportion of
exemplars recalled was the dependent variable. We
found a significant main effect for retrieval type, F(1,
458) = 119.25, p < .001, ηp
2 = .21, but no significant main
effect for risk condition, F(1, 458) = 2.28, p = .13, ηp
2 =
.005, and no significant interaction between risk condition
and retrieval type, F(1, 458) = 0.11, p = .74, ηp
2 =
.001. In exploring the main effect for retrieval type, we
found a significant difference between recall of Rp+ items
(M = .66, SD = .31) and recall of Nrp items (M = .52,
SD = .25) in the low-risk condition, t(226) = 7.54, p <
.001, d = 0.50, 95% CI for the difference between retrieval
types = [.11, .19]. Similarly, in the high-risk condition,
recall of Rp+ items (M = .69, SD = .30) was significantly
higher than recall of Nrp items (M = .55, SD = .24),
t(232) = 7.93, p < .001, d = 0.52, 95% CI for the difference
between retrieval types = [.10, .17] (see Fig. 1). The magnitude
of the rehearsal effect was not significantly different
between the high-risk (M = .13, SD = .27) and the
low-risk (M = .15, SD = .30) conditions, t(458) = 0.33, p =
.74, d = 0.04, 95% CI for the difference between conditions
= [−.04, .07] (see Fig. 2).

------

[PILOT/COPILOT DO NOT CHANGE THE CODE IN THE CHUNK BELOW]  

```{r global_options, include=FALSE}
# sets up some formatting options for the R Markdown document
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

# Step 1: Load packages and prepare report object

```{r}
# load packages
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CARPSreports) # custom report functions
library(foreign) #read SPSS file 
char = as.character
num = function(x) {return (as.numeric(char(x)))}
library(psycho) #may not be needed
library(sjstats)
```

[PILOT/COPILOT DO NOT MAKE CHANGES TO THE CODE CHUNK BELOW]

```{r}
# Prepare report object. This will be updated automatically by the reproCheck function each time values are compared
reportObject <- data.frame(dummyRow = TRUE, reportedValue = NA, obtainedValue = NA, valueType = NA, percentageError = NA, comparisonOutcome = NA, eyeballCheck = NA)
```

# Step 2: Load data

```{r}
df = read_spss("../CARPS_EXT_26-10-2015-master/data/Infectious_Cognition-Dataset_for_public_upload.sav")
#460 participants in dataset, which signals exclusions have arleady been made 
```

# Step 3: Tidy data

```{r}
df = df %>% 
  gather("recall_type", "recall_proportion", `RPplus#Recall`, `RPminus#Recall`, `NRP#Recall`) %>% 
  mutate(recall_proportion = num(recall_proportion)) %>% 
  separate (recall_type, c("recall_type",NA), sep = "#", remove = TRUE)
```

# Step 4: Run analysis

## Descriptive statistics

```{r}
```

## Inferential statistics

"In the analysis testing for a rehearsal effect, risk condition was a between-subjects variable, retrieval type (Rp+ vs. Nrp) was a within-subjects variable, and  proportion of exemplars recalled was the dependent variable. We found a significant main effect for retrieval type, F(1, 458) = 119.25, p < .001, ηp 2 = .21, but no significant main effect for risk condition, F(1, 458) = 2.28, p = .13, ηp 2 = 2 .005, and no significant interaction between risk condition and retrieval type, F(1, 458) = 0.11, p = .74, ηp 2 = 2 .001."
```{r}
#subsetting the data, selecting "RPplus" and "NRP" for rehersal effect
df_subset = df %>% 
  select(`PARTICIPANT#ID`, RISKCONDITION, recall_proportion, recall_type) %>% 
  filter(recall_type %in% c("NRP","RPplus"))

#repeated measures mixed (between and within) ANOVA
summary(df_aov <- aov(recall_proportion ~ recall_type + RISKCONDITION + RISKCONDITION*recall_type + Error(`PARTICIPANT#ID`/RISKCONDITION), data = df_subset))

#eta squared calculations 
eta_sq(df_aov)
```
"In exploring the main effect for retrieval type, we found a significant difference between recall of Rp+ items (M = .66, SD = .31) and recall of Nrp items (M = .52, SD = .25) in the low-risk condition, t(226) = 7.54, p < .001, d = 0.50, 95% CI for the difference between retrieval types = [.11, .19]."

```{r}
#select low-risk condition 
df_subset_low = df_subset %>% 
  filter(RISKCONDITION %in% c("1"))

#t test 
t.test(df_subset_low$recall_proportion ~ df_subset_low$recall_type)


#RPplus means and SD
df_subset_low_RPplus = df_subset_low %>% 
  filter(recall_type %in% c("RPplus"))
mean(df_subset_low_RPplus$recall_proportion)
sd(df_subset_low_RPplus$recall_proportion)

#NRP means and SD
df_subset_low_NFP = df_subset_low %>% 
  filter(recall_type %in% c("NRP"))
mean(df_subset_low_NFP$recall_proportion)
sd(df_subset_low_NFP$recall_proportion)
```

"Similarly, in the high-risk condition, recall of Rp+ items (M = .69, SD = .30) was significantly higher than recall of Nrp items (M = .55, SD = .24), t(232) = 7.93, p < .001, d = 0.52, 95% CI for the difference between retrieval types = [.10, .17]"
```{r}
#select high-risk condition 
df_subset_high = df_subset %>% 
  filter(RISKCONDITION %in% c("2"))

#within subjects ANOVA ## NEED THIS STILL 
summary(df_aov <- aov(recall_proportion ~ RISKCONDITION + recall_type + RISKCONDITION*recall_type + Error(`PARTICIPANT#ID`/RISKCONDITION), data = df_subset))

#RPplus means and SD
df_subset_high_RPplus = df_subset_high %>% 
  filter(recall_type %in% c("RPplus"))
mean(df_subset_high_RPplus$recall_proportion)
sd(df_subset_high_RPplus$recall_proportion)

#NRP means and SD
df_subset_high_NFP = df_subset_high %>% 
  filter(recall_type %in% c("NRP"))
mean(df_subset_high_NFP$recall_proportion)
sd(df_subset_high_NFP$recall_proportion)

```

"The magnitude of the rehearsal effect was not significantly different between the high-risk (M = .13, SD = .27) and the low-risk (M = .15, SD = .30) conditions, t(458) = 0.33, p = .74, d = 0.04, 95% CI for the difference between conditions = [−.04, .07]"

```{r}

```


```{r}
#graphs
```


# Step 5: Conclusion

[Please include a text summary describing your findings. If this reproducibility check was a failure, you should note any suggestions as to what you think the likely cause(s) might be.]
  
[PILOT/COPILOT ENTER RELEVANT INFORMATION BELOW]

```{r}
Author_Assistance = FALSE # was author assistance provided? (if so, enter TRUE)
Insufficient_Information_Errors <- 0 # how many discrete insufficient information issues did you encounter?
# Assess the causal locus (discrete reproducibility issues) of any reproducibility errors. Note that there doesn't necessarily have to be a one-to-one correspondance between discrete reproducibility issues and reproducibility errors. For example, it could be that the original article neglects to mention that a Greenhouse-Geisser correct was applied to ANOVA outcomes. This might result in multiple reproducibility errors, but there is a single causal locus (discrete reproducibility issue).
locus_typo <- NA # how many discrete issues did you encounter that related to typographical errors?
locus_specification <- NA # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis <- NA # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data <- NA # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified <- NA # how many discrete issues were there for which you could not identify the cause
# How many of the above issues were resolved through author assistance?
locus_typo_resolved <- NA # how many discrete issues did you encounter that related to typographical errors?
locus_specification_resolved <- NA # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis_resolved <- NA # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data_resolved <- NA # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified_resolved <- NA # how many discrete issues were there for which you could not identify the cause
Affects_Conclusion <- NA # Do any reproducibility issues encounter appear to affect the conclusions made in the original article? TRUE, FALSE, or NA. This is a subjective judgement, but you should taking into account multiple factors, such as the presence/absence of decision errors, the number of target outcomes that could not be reproduced, the type of outcomes that could or could not be reproduced, the difference in magnitude of effect sizes, and the predictions of the specific hypothesis under scrutiny.
```

[PILOT/COPILOT DOD NOT EDIT THE CODE CHUNK BELOW]

```{r}
reportObject <- reportObject %>%
  filter(dummyRow == FALSE) %>% # remove the dummy row
  select(-dummyRow) %>% # remove dummy row designation
  mutate(articleID = articleID) %>% # add variables to report 
  select(articleID, everything()) # make articleID first column
# decide on final outcome
if(any(reportObject$comparisonOutcome %in% c("MAJOR_ERROR", "DECISION_ERROR")) | Insufficient_Information_Errors > 0){
  finalOutcome <- "Failure without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Failure despite author assistance"
  }
}else{
  finalOutcome <- "Success without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Success with author assistance"
  }
}
# collate report extra details
reportExtras <- data.frame(articleID, pilotNames, copilotNames, pilotTTC, copilotTTC, pilotStartDate, copilotStartDate, completionDate, Author_Assistance, finalOutcome, Insufficient_Information_Errors, locus_typo, locus_specification, locus_analysis, locus_data, locus_unidentified, locus_typo_resolved, locus_specification_resolved, locus_analysis_resolved, locus_data_resolved, locus_unidentified_resolved)
# save report objects
if(reportType == "pilot"){
  write_csv(reportObject, "pilotReportDetailed.csv")
  write_csv(reportExtras, "pilotReportExtras.csv")
}
if(reportType == "final"){
  write_csv(reportObject, "finalReportDetailed.csv")
  write_csv(reportExtras, "finalReportExtras.csv")
}
```

# Session information

[This function will output information about the package versions used in this report:]

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```