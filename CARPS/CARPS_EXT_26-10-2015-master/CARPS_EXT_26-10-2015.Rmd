---
title: "CARPS Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

# Report Details

```{r}
articleID <- "26-10-2015" # insert the article ID code here e.g., "10-3-2015_PS"
reportType <- "pilot" # specify whether this is the 'pilot' report or 'final' report
pilotNames <- "Ellen Reinhart" # insert the pilot's name here e.g., "Tom Hardwicke".  If there are multiple cpilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
copilotNames <- "Griffin Dietz" # # insert the co-pilot's name here e.g., "Michael Frank". If there are multiple co-pilots enter both names in a character string e.g., "Tom Hardwicke, Bob Dylan"
pilotTTC <- 540 # insert the pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
copilotTTC <- 60 # insert the co-pilot's estimated time to complete (in minutes, fine to approximate) e.g., 120
pilotStartDate <- 10/30/2018 # insert the pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
copilotStartDate <- 10/30/2018 # insert the co-pilot's start date in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
completionDate <- NA # copilot insert the date of final report completion (after any necessary rounds of author assistance) in US format e.g., as.Date("01/25/18", format = "%m/%d/%y")
```

------

#### Methods summary: 

Participants completed a brief knowledge quiz on meningococcal disease. They then read 16 category-exemplary statements from four categories (risk factors, symptoms, test and diagnosis, and after effects). After, participants were randomly assigned to read either a high-threat or low-threat message about the disease and then completed a manipulation check. Next, participants listened to an audio clip in the "retrieval-practice phase." During this phase, participants were randomly assigned to 1 of 8 conditions that counterbalanced such that the audio clip mentioned 2 facts from 2 categories and did not mention the other 2 facts in each of those categories nor any facts from the other two categories. After completing a distractor questionnaire, participants completed the cued-recall task. During this task, participants were presented with the titles of the categories and ask to record every fact they could remember. A rater coded the open-ended responses for accuracy. 

------

#### Target outcomes: 

> Inasmuch as our hypothesis was focused on concurrent retrieval processes that primarily affect SSRIF, we conducted separate repeated measures analyses of variance to test for rehearsal effects and for SSRIF effects. In the analysis testing for a rehearsal effect, risk condition was a between-subjects variable, retrieval type (Rp+ vs. Nrp) was a within-subjects variable, and proportion of exemplars recalled was the dependent variable. We found a significant main effect for retrieval type, F(1, 458) = 119.25, p < .001, ηp 2 = .21, but no  significant main effect for risk condition, F(1, 458) = 2.28, p = .13, ηp 2 = .005, and no significant interaction between risk condition and retrieval type, F(1, 458) = 0.11, p = .74, ηp 2 = .001. In exploring the main effect for retrieval type, we found a significant difference between recall of Rp+ items (M = .66, SD = .31) and recall of Nrp items (M = .52, SD = .25) in the low-risk condition, t(226) = 7.54, p <
.001, d = 0.50, 95% CI for the difference between retrieval types = [.11, .19]. Similarly, in the high-risk condition, recall of Rp+ items (M = .69, SD = .30) was significantly higher than recall of Nrp items (M = .55, SD = .24), t(232) = 7.93, p < .001, d = 0.52, 95% CI for the difference between retrieval types = [.10, .17] (see Fig. 1). The magnitude of the rehearsal effect was not significantly different between the high-risk (M = .13, SD = .27) and the low-risk (M = .15, SD = .30) conditions, t(458) = 0.33, p = .74, d = 0.04, 95% CI for the difference between conditions = [−.04, .07] (see Fig. 2).
------

```{r global_options, include=FALSE}
# sets up some formatting options for the R Markdown document
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

# Step 1: Load packages and prepare report object

```{r}
# load packages
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CARPSreports) # custom report functions
library(foreign) #read SPSS file 
char = as.character
num = function(x) {return (as.numeric(char(x)))}
library(sjstats)
library(effsize)
library(devtools)
```

```{r}
# Prepare report object. This will be updated automatically by the reproCheck function each time values are compared
reportObject <- data.frame(dummyRow = TRUE, reportedValue = NA, obtainedValue = NA, valueType = NA, percentageError = NA, comparisonOutcome = NA, eyeballCheck = NA)
```

# Step 2: Load data

```{r}
df = read_spss("../CARPS_EXT_26-10-2015-master/data/Infectious_Cognition-Dataset_for_public_upload.sav")
#460 participants in the dataset, which signals exclusions have already been made 
```

# Step 3: Tidy data

```{r}
df = df %>% 
  gather("recall_type", "recall_proportion", `RPplus#Recall`, `RPminus#Recall`, `NRP#Recall`) %>% 
  mutate(recall_proportion = num(recall_proportion)) %>% 
  separate (recall_type, c("recall_type",NA), sep = "#", remove = TRUE)
```

# Step 4: Run analysis

## Inferential statistics

"In the analysis testing for a rehearsal effect, risk condition was a between-subjects variable, retrieval type (Rp+ vs. Nrp) was a within-subjects variable, and  proportion of exemplars recalled was the dependent variable. We found a significant main effect for retrieval type, F(1, 458) = 119.25, p < .001, ηp 2 = .21, but no significant main effect for risk condition, F(1, 458) = 2.28, p = .13, ηp 2 = 2 .005, and no significant interaction between risk condition and retrieval type, F(1, 458) = 0.11, p = .74, ηp 2 = 2 .001."
```{r}
#subsetting the data, selecting "RPplus" and "NRP" for rehearsal effect
df_subset = df %>% 
  select(`PARTICIPANT#ID`, RISKCONDITION, recall_proportion, recall_type, `Practice#effect`) %>% 
  filter(recall_type %in% c("NRP","RPplus"))

#repeated measures mixed (between and within) ANOVA
summary(df_aov <- aov(recall_proportion ~ RISKCONDITION*recall_type + Error(`PARTICIPANT#ID`/recall_type), data=df_subset))

#eta squared  
eta_sq(df_aov)
eta_recall_check <- reproCheck('.21', '.050', valueType = c("other"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #NON-MANUAL CHECK NEEDED #MAJOR ERROR
eta_risk_check <- reproCheck('.005', '.000', valueType = c("other"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #NON-MANUAL CHECK NEEDED #MAJOR ERROR
eta_interaction_check <- reproCheck('.001', '.000', valueType = c("other"), eyeballCheck = TRUE,
  round = TRUE, updatedReportObject = reportObject) #NON-MANUAL CHECK NEEDED #MATCH
```

"In exploring the main effect for retrieval type, we found a significant difference between recall of Rp+ items (M = .66, SD = .31) and recall of Nrp items (M = .52, SD = .25) in the low-risk condition, t(226) = 7.54, p < .001, d = 0.50, 95% CI for the difference between retrieval types = [.11, .19]."

```{r}
#select low-risk condition 
df_subset_low = df_subset %>% 
  filter(RISKCONDITION %in% c("1"))

#t test 
t.test(df_subset_low$recall_proportion ~ df_subset_low$recall_type, paired=TRUE)
low_t_t_check <- reproCheck(reportedValue = "7.54", obtainedValue = "-7.55", valueType = c("t"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #NON-MANUAL CHECK NEEDED #MAJOR ERROR
low_t_p_check <- reproCheck(reportedValue = ".001", obtainedValue = "1.064e-12", valueType = c("p"), eyeballCheck = TRUE,
  round = TRUE, updatedReportObject = reportObject) #NON-MANUAL CHECK NEEDED #MATCH

#effect size
cohen.d(df_subset_low$recall_proportion ~ df_subset_low$recall_type)
low_d_check <- reproCheck(reportedValue = ".50", obtainedValue = ".538499", valueType = c("other"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #NON-MANUAL CHECK NEEDED #MINOR ERROR

#RPplus low risk mean
df_subset_low_RPplus = df_subset_low %>% 
  filter(recall_type %in% c("RPplus"))
RPplus_low_mean <- mean(df_subset_low_RPplus$recall_proportion)
RPplus_low_mean_check <- reproCheck('.66',RPplus_low_mean, valueType = c("mean"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #MINOR ERROR

#RPplus low risk SD
RPplus_low_sd <- sd(df_subset_low_RPplus$recall_proportion)
RPplus_low_sd_check <- reproCheck('.31', RPplus_low_sd, valueType = c("sd"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #MINOR ERROR

#NRP low risk mean
df_subset_low_NFP = df_subset_low %>% 
  filter(recall_type %in% c("NRP"))
NRP_low_mean <- mean(df_subset_low_NFP$recall_proportion)
NRP_low_mean_check <- reproCheck('.52',NRP_low_mean, valueType = c("mean"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #MATCH

#NRP low risk SD
NRP_low_sd <- sd(df_subset_low_NFP$recall_proportion)
NRP_low_sd_check <- reproCheck('.25', NRP_low_sd, valueType = c("sd"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #MATCH
```

"Similarly, in the high-risk condition, recall of Rp+ items (M = .69, SD = .30) was significantly higher than recall of Nrp items (M = .55, SD = .24), t(232) = 7.93, p < .001, d = 0.52, 95% CI for the difference between retrieval types = [.10, .17]"
```{r}
#select high-risk condition 
df_subset_high = df_subset %>% 
  filter(RISKCONDITION %in% c("2"))

#t test 
t.test(df_subset_high$recall_proportion ~ df_subset_high$recall_type, paired=TRUE)
high_t_t_check <- reproCheck(reportedValue = "7.93", obtainedValue = "-7.6486", valueType = c("t"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #NON-MANUAL CHECK NEEDED #MAJOR ERROR

#effect size
cohen.d(df_subset_high$recall_proportion ~ df_subset_high$recall_type)
high_d_check <- reproCheck(reportedValue = ".52", obtainedValue = ".5006027", valueType = c("other"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #NON-MANUAL CHECK NEEDED #MINOR ERROR

#RPplus high risk mean 
df_subset_high_RPplus = df_subset_high %>% 
  filter(recall_type %in% c("RPplus"))
RPplus_high_mean <- mean(df_subset_high_RPplus$recall_proportion)
RPplus_high_mean_check <- reproCheck('.69',RPplus_high_mean, valueType = c("mean"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #MATCH

#RPplus high risk SD 
RPplus_high_sd <- sd(df_subset_high_RPplus$recall_proportion)
RPplus_high_sd_check <- reproCheck('.30',RPplus_high_sd, valueType = c("sd"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #MATCH

#NRP high risk mean
df_subset_high_NFP = df_subset_high %>% 
  filter(recall_type %in% c("NRP"))
NRP_high_mean <- mean(df_subset_high_NFP$recall_proportion)
NRP_high_mean_check <- reproCheck('.55',NRP_high_mean, valueType = c("mean"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #MATCH

#NRP high risk SD
NRP_high_sd <- sd(df_subset_high_NFP$recall_proportion)
NRP_high_sd_check <- reproCheck('.24', NRP_high_sd, valueType = c("sd"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #MATCH
```

"The magnitude of the rehearsal effect was not significantly different between the high-risk (M = .13, SD = .27) and the low-risk (M = .15, SD = .30) conditions, t(458) = 0.33, p = .74, d = 0.04, 95% CI for the difference between conditions = [−.04, .07]"

```{r}
#t test 
t.test(df_subset$`Practice#effect` ~ df_subset$RISKCONDITION)
practice_t_t_check <- reproCheck(reportedValue = ".33", obtainedValue = "0.89253", valueType = c("t"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #NON-MANUAL CHECK NEEDED #MAJOR ERROR
practice_t_p_check <- reproCheck(reportedValue = ".74", obtainedValue = ".3723", valueType = c("p"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #NON-MANUAL CHECK NEEDED #MAJOR ERROR

#effect size 
cohen.d(df_subset$`Practice#effect` ~ df_subset$RISKCONDITION) 
practice_d_check <- reproCheck(reportedValue = ".04", obtainedValue = ".05894662", valueType = c("other"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #NON-MANUAL CHECK NEEDED #MAJOR ERROR

#Practice effect high risk condition mean
practice_high_mean <- mean(df_subset_high$`Practice#effect`)
practice_high_mean_check <- reproCheck('.13', practice_high_mean, valueType = c("mean"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #MATCH

#Practice effect high risk condition SD
practice_high_sd <- sd(df_subset_high$`Practice#effect`)
practice_high_sd_check <- reproCheck('.27', practice_high_sd, valueType = c("sd"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #MATCH

#Practice effect low risk condition mean
practice_low_mean <- mean(df_subset_low$`Practice#effect`)
practice_low_mean_check <- reproCheck('.15', practice_low_mean, valueType = c("mean"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #MATCH

#Practice effect low risk condition SD 
practice_low_sd <- sd(df_subset_low$`Practice#effect`)
practice_low_mean_check <- reproCheck('.30', practice_low_sd, valueType = c("sd"), eyeballCheck = NA,
  round = TRUE, updatedReportObject = reportObject) #MATCH


```

# Step 5: Conclusion

Twelve numbers matched the original report. Four had major discrepancies and seven had minor discrepancies. The means and standard deviations matched, and significance tests often did not. This suggests that the data provided are the same as was used in the original analyses, but the analysis procedure is different. Overall, the data follow the conclusions drawn from the article. 

```{r}
Author_Assistance = FALSE # was author assistance provided? (if so, enter TRUE)
Insufficient_Information_Errors <- 0 # how many discrete insufficient information issues did you encounter?
# Assess the causal locus (discrete reproducibility issues) of any reproducibility errors. Note that there doesn't necessarily have to be a one-to-one correspondance between discrete reproducibility issues and reproducibility errors. For example, it could be that the original article neglects to mention that a Greenhouse-Geisser correct was applied to ANOVA outcomes. This might result in multiple reproducibility errors, but there is a single causal locus (discrete reproducibility issue).
locus_typo <- 0 # how many discrete issues did you encounter that related to typographical errors?
locus_specification <- 0 # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis <- 0 # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data <- 0 # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified <- 11 # how many discrete issues were there for which you could not identify the cause
# How many of the above issues were resolved through author assistance?
locus_typo_resolved <- 0 # how many discrete issues did you encounter that related to typographical errors?
locus_specification_resolved <- 0 # how many discrete issues did you encounter that related to incomplete, incorrect, or unclear specification of the original analyses?
locus_analysis_resolved <- 0 # how many discrete issues did you encounter that related to errors in the authors' original analyses?
locus_data_resolved <- 0 # how many discrete issues did you encounter that related to errors in the data files shared by the authors?
locus_unidentified_resolved <- 0 # how many discrete issues were there for which you could not identify the cause
Affects_Conclusion <- FALSE # Do any reproducibility issues encounter appear to affect the conclusions made in the original article? TRUE, FALSE, or NA. This is a subjective judgement, but you should taking into account multiple factors, such as the presence/absence of decision errors, the number of target outcomes that could not be reproduced, the type of outcomes that could or could not be reproduced, the difference in magnitude of effect sizes, and the predictions of the specific hypothesis under scrutiny.
```


```{r}
reportObject <- reportObject %>%
  filter(dummyRow == FALSE) %>% # remove the dummy row
  select(-dummyRow) %>% # remove dummy row designation
  mutate(articleID = articleID) %>% # add variables to report 
  select(articleID, everything()) # make articleID first column
# decide on final outcome
if(any(reportObject$comparisonOutcome %in% c("MAJOR_ERROR", "DECISION_ERROR")) | Insufficient_Information_Errors > 0){
  finalOutcome <- "Failure without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Failure despite author assistance"
  }
}else{
  finalOutcome <- "Success without author assistance"
  if(Author_Assistance == T){
    finalOutcome <- "Success with author assistance"
  }
}
# collate report extra details
reportExtras <- data.frame(articleID, pilotNames, copilotNames, pilotTTC, copilotTTC, pilotStartDate, copilotStartDate, completionDate, Author_Assistance, finalOutcome, Insufficient_Information_Errors, locus_typo, locus_specification, locus_analysis, locus_data, locus_unidentified, locus_typo_resolved, locus_specification_resolved, locus_analysis_resolved, locus_data_resolved, locus_unidentified_resolved)
# save report objects
if(reportType == "pilot"){
  write_csv(reportObject, "pilotReportDetailed.csv")
  write_csv(reportExtras, "pilotReportExtras.csv")
}
if(reportType == "final"){
  write_csv(reportObject, "finalReportDetailed.csv")
  write_csv(reportExtras, "finalReportExtras.csv")
}
```

# Session information

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```